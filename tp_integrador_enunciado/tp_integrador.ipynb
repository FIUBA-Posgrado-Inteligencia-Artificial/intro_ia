{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo práctico integrador\n",
        "\n",
        "**Nombre**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa76473",
      "metadata": {},
      "source": [
        "## Primera Parte (Clase 1 y 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aaf94e0a",
      "metadata": {
        "id": "aaf94e0a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffe9554",
      "metadata": {},
      "source": [
        "### Primer ejercicio\n",
        "\n",
        "Dada una matriz en formato *numpy array*, donde cada fila de la matriz representa un vector matemático, se requiere computar las normas $l_0$, $l_1$, $l_2$, $l_{\\infty}$, según la siguientes definiciones:\n",
        "\n",
        "\\begin{equation}\n",
        "    ||\\mathbf{x}||^{p} = \\bigg(\\sum_{j=1}^{n}{|x_i|^p}\\bigg)^{\\frac{1}{p}}\n",
        "\\end{equation}\n",
        "\n",
        "con los casos especiales para $p=0$ y $p=\\infty$ siendo:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{array}{rcl}\n",
        "        ||\\mathbf{x}||_0 & = & \\bigg(\\sum_{j=1 \\wedge x_j != 0}{|x_i|}\\bigg)\\\\\n",
        "        ||\\mathbf{x}||_{\\infty} & = & \\max_{i}{|x_i|}\\\\\n",
        "    \\end{array}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0bdb0ee3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norma l0:  [3 3 0]\n",
            "Norma l1:  [ 6. 15.  0.]\n",
            "Norma l2:  [3.74165739 8.77496439 0.        ]\n",
            "Norma linfinito:  [3 6 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creamos una matriz de ejemplo\n",
        "matriz = np.array([[1, 2, 3], [4, 5, 6], [0, 0, 0]])\n",
        "\n",
        "# Calculamos las normas\n",
        "n_l0 = np.count_nonzero(matriz, axis=1)\n",
        "n_l1 = np.linalg.norm(matriz, ord=1, axis=1)\n",
        "n_l2 = np.linalg.norm(matriz, ord=2, axis=1)\n",
        "n_linfinito = np.max(np.abs(matriz), axis=1)\n",
        "\n",
        "# Imprimimos los resultados\n",
        "print(\"Norma l0: \", n_l0)\n",
        "print(\"Norma l1: \", n_l1)\n",
        "print(\"Norma l2: \", n_l2)\n",
        "print(\"Norma linfinito: \", n_linfinito)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd66d862",
      "metadata": {},
      "source": [
        "### Segundo Ejercicio\n",
        "\n",
        "En clasificación contamos con dos arreglos, la “verdad” y la “predicción”. Cada elemento de los arreglos pueden tomar dos valores, “True” (representado por 1) y “False” (representado por 0). Entonces podemos definir 4 variables:\n",
        "\n",
        "* True Positive (TP): El valor verdadero es 1 y el valor predicho es 1\n",
        "* True Negative (TN): El valor verdadero es 0 y el valor predicho es 0\n",
        "* False Positive (FP): El valor verdadero es 0 y el valor predicho es 1\n",
        "* False Negative (FN): El valor verdadero es 1 y el valor predicho es 0\n",
        "\n",
        "A partir de esto definimos:\n",
        "\n",
        "* Precision = TP / (TP + FP)\n",
        "* Recall = TP / (TP + FN)\n",
        "* Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        " \n",
        "Calcular las 3 métricas con Numpy y operaciones vectorizadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "794dcd58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TP: 3       \n",
            "TN: 1,       \n",
            "FP: 3,       \n",
            "FN: 3,       \n",
            "Precision: 0.50,       \n",
            "Recall: 0.50,       \n",
            "Accuracy: 0.40\n"
          ]
        }
      ],
      "source": [
        "truth = np.array([1,1,0,1,1,1,0,0,0,1])\n",
        "prediction = np.array([1,1,1,1,0,0,1,1,0,0])\n",
        "\n",
        "TP =np.sum((truth==1)&(prediction==1))\n",
        "TN =np.sum((truth==0)&(prediction==0))\n",
        "FP =np.sum((truth==0)&(prediction==1))\n",
        "FN =np.sum((truth==1)&(prediction==0))\n",
        "Precision=TP/(TP+FP)\n",
        "Recall=TP/(TP+FN)\n",
        "Accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
        "print('TP: {0} \\\n",
        "      \\nTN: {1} \\\n",
        "      \\nFP: {2} \\\n",
        "      \\nFN: {3} \\\n",
        "      \\nPrecision: {4:.2f} \\\n",
        "      \\nRecall: {5:.2f} \\\n",
        "      \\nAccuracy: {6:.2f}'.format(TP, TN, FP, FN, Precision, Recall, Accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612a03e6",
      "metadata": {},
      "source": [
        "### Tercer y Cuarto Ejercicio\n",
        "\n",
        "Para este ejercicio vamos a considerar los siguientes datasets:\n",
        "\n",
        "* [HAR](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) (Ejercicio 3)\n",
        "* [MNIST](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) (Ejercicio 4)\n",
        "\n",
        "1. Aplicar PCA (validar que se cumplan las condiciones), ¿Cuántas componentes necesitamos para explicar el 80% de la varianza?\n",
        "2. Gráficar la variación acumulada para cada caso.\n",
        "3. Utilizando [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Agrupar el dataset transformado (ejercicio de PCA) y agrupar en clusters de $k=6$ (ej 3) y $k=10$ (ej 4). Luego en ambos casos probar con $k=2$.\n",
        "4. Graficar los resultados con los distintos k's usando las primeras dos componentes principales como ejes x,y.\n",
        "5. Explique. ¿Cuál fue la ganancia de usar PCA en conjunto con k-means?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffbe422",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7c7574fd",
      "metadata": {},
      "source": [
        "## Segunda Parte (Clase 3 y 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aaf94e0a",
      "metadata": {
        "id": "aaf94e0a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento de modelos de prueba\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Evaluación de modelos de prueba\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Crear datasets\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z4CnDndvBlc6",
      "metadata": {
        "id": "z4CnDndvBlc6"
      },
      "source": [
        "Vamos a crear un dataset sintetico utilizando las librerias de [Sklearn Datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html), en particular nos interesa crear un problema de regresion\n",
        "lineal al que podemos variarle sus parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65843123",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65843123",
        "outputId": "6589754e-2630-4c3d-cd45-b5f10ad8dc49"
      },
      "outputs": [],
      "source": [
        "# Creamos un dataset de prueba\n",
        "X, y = make_regression(n_samples = 1000,\n",
        "                       n_features = 1,\n",
        "                       noise = 2,\n",
        "                       n_informative = 1,\n",
        "                       random_state = 42)\n",
        "\n",
        "new_data = np.append(X,y.reshape(-1,1),axis=1)\n",
        "new_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52eSwfzCslx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "a52eSwfzCslx",
        "outputId": "51d2ef98-ff3b-4182-966e-a03b1a14ba1a"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(new_data)\n",
        "\n",
        "new_col = []\n",
        "i = 0\n",
        "for col in df.columns:\n",
        "    if i ==  len(df.columns) - 1:\n",
        "        new_col.append(\"target\")\n",
        "    else :\n",
        "        new_col.append(\"feature_\" + str(i+1))\n",
        "        \n",
        "    i += 1\n",
        "    \n",
        "df.columns = new_col\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CBb_QmSZCPFZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "CBb_QmSZCPFZ",
        "outputId": "a7c8b051-581d-4267-8bad-5db770be6740"
      },
      "outputs": [],
      "source": [
        "fig,axes = plt.subplots(2,figsize=(22,6))\n",
        "axes[0].scatter(X,y)\n",
        "sns.histplot(X, ax=axes[1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zTvu6e3KhEKG",
      "metadata": {
        "id": "zTvu6e3KhEKG"
      },
      "source": [
        "### Funciones auxiliares para generar datos anómalos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_LqDjS8chDkY",
      "metadata": {
        "id": "_LqDjS8chDkY"
      },
      "outputs": [],
      "source": [
        "def generate_outliers(df: pd.DataFrame,\n",
        "                     cols: list = None,\n",
        "                     extreme_outlier: bool = False,\n",
        "                     two_tailed: bool = False,\n",
        "                     percentage: float = 0.02) -> pd.DataFrame:\n",
        "  \"\"\"Con esta función vamos a poder generar outliers en ciertas columnas de nuestro\n",
        "  dataset. Si le damos True a _extreme_outlier_ va a generar outliers con mucho\n",
        "  peso en la regresión (puede ser bilateral o unilateral segun _two_tailed_)\n",
        "  \"\"\"  \n",
        "  seeds = np.random.randint(100, size = len(df))\n",
        "  \n",
        "  nsamples = np.math.floor(len(df) * percentage)\n",
        "  idx_to_change = df.sample(n = nsamples).index\n",
        "\n",
        "  cols = df.columns.tolist() if cols is None else cols\n",
        "\n",
        "  result = df.copy(deep = True)\n",
        "  \n",
        "  for i,col_name in enumerate(cols):\n",
        "    np.random.seed(seeds[i])\n",
        "\n",
        "    iqr = result[col_name].quantile(0.75) - result[col_name].quantile(0.25)\n",
        "\n",
        "    lb = result[col_name].quantile(0.25) - 1 * iqr\n",
        "    ub = result[col_name].quantile(0.75) + 1 * iqr\n",
        "\n",
        "    if two_tailed:\n",
        "      outs = result[col_name].loc[(result[col_name] < lb) | (result[col_name] > ub)]\n",
        "    else:\n",
        "      outs = result[col_name].loc[(result[col_name] > ub)]\n",
        "    \n",
        "    out_size = len(outs)\n",
        "    if out_size < nsamples:\n",
        "      nsamples = out_size\n",
        "\n",
        "    idx_to_change = outs.sample(nsamples, replace = False).index\n",
        "    \n",
        "    if extreme_outlier:\n",
        "      outlier_sign = [1 if np.random.random() < 0.9 else -1 for _ in range(nsamples)]\n",
        "      \n",
        "      result[col_name].loc[idx_to_change] = np.multiply(outlier_sign,\n",
        "                                                        np.random.uniform(low = result[col_name].mean(),\n",
        "                                                                          high = result[col_name].max()*5,\n",
        "                                                                          size = nsamples)\n",
        "                                                        )\n",
        "      result['target'].loc[idx_to_change] = np.multiply(outlier_sign,\n",
        "                                                        np.random.uniform(low = result['target'].mean(),\n",
        "                                                                          high = result['target'].max()*2,\n",
        "                                                                          size = nsamples)\n",
        "                                                        )\n",
        "    else:\n",
        "      samples = result[col_name].loc[idx_to_change].values\n",
        "      np.random.shuffle(samples)\n",
        "      result[col_name].loc[idx_to_change] = samples\n",
        "  \n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mmnwCbF1gugP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmnwCbF1gugP",
        "outputId": "a27810fe-1f0f-40ef-99ac-dace0fc0eb92"
      },
      "outputs": [],
      "source": [
        "df_outlier = generate_outliers(df,['feature_1'], percentage = 0.05, extreme_outlier = True, two_tailed= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b520928",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.lmplot(data = pd.concat([df.assign(dataset = 'set1'),\n",
        "                                 df_outlier.assign(dataset = 'set2')]),\n",
        "                x = 'feature_1',\n",
        "                y = 'target',\n",
        "                hue = 'dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N_XkGtvw7J-i",
      "metadata": {
        "id": "N_XkGtvw7J-i"
      },
      "source": [
        "### Quinto ejercicio\n",
        "\n",
        "Crear una función que separe los datos en train-validation-test 70-20-10\n",
        "\n",
        "\n",
        "Hints: \n",
        "\n",
        "* Usar Indexing y slicing\n",
        "* Usar np.random.[...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yIOYdDHz7Jkp",
      "metadata": {
        "id": "yIOYdDHz7Jkp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split(df: np.ndarray):\n",
        "    \n",
        "    return X_train, X_val, X_test, Y_train, Y_val, Y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b77ad81a",
      "metadata": {
        "id": "b77ad81a"
      },
      "source": [
        "### Sexto ejercicio\n",
        "\n",
        "Utilizando la funcion `generate_outliers` generar puntos extremos dentro de los datos que generamos anteriormente. En este ejercicio dejar setteado `extreme_outliers` como `False` y observe como variando el porcentaje de los mismos la regresión comienza a afectarse.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Generar datasets:\n",
        "    - Uno normal con poco `noise` y pocos outliers\n",
        "    - Uno con mucho `noise` y pocos outliers\n",
        "    - Uno con poco `noise` y muchos outliers\n",
        "    - Uno con mucho `noise` y muchos outliers\n",
        "2. Probar los distintos regresores a ver como se comportan frente a estos datasets anómalos.\n",
        "3. Comparar y analizar resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "79dcb2cc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.123429</td>\n",
              "      <td>-1.253402</td>\n",
              "      <td>0.370340</td>\n",
              "      <td>0.101788</td>\n",
              "      <td>0.092628</td>\n",
              "      <td>-0.589254</td>\n",
              "      <td>0.306348</td>\n",
              "      <td>-1.458213</td>\n",
              "      <td>1.630130</td>\n",
              "      <td>1.242863</td>\n",
              "      <td>-44.755165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.216827</td>\n",
              "      <td>0.214983</td>\n",
              "      <td>-0.028817</td>\n",
              "      <td>-1.701140</td>\n",
              "      <td>0.264482</td>\n",
              "      <td>0.314972</td>\n",
              "      <td>0.374062</td>\n",
              "      <td>-0.292758</td>\n",
              "      <td>0.501900</td>\n",
              "      <td>0.063702</td>\n",
              "      <td>-16.218565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.388979</td>\n",
              "      <td>1.655407</td>\n",
              "      <td>-0.755792</td>\n",
              "      <td>-1.161784</td>\n",
              "      <td>-0.300860</td>\n",
              "      <td>1.048707</td>\n",
              "      <td>-0.283139</td>\n",
              "      <td>0.251474</td>\n",
              "      <td>-0.194269</td>\n",
              "      <td>-1.209477</td>\n",
              "      <td>-9.541669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.219072</td>\n",
              "      <td>-0.251552</td>\n",
              "      <td>-1.095871</td>\n",
              "      <td>-0.806520</td>\n",
              "      <td>-0.435139</td>\n",
              "      <td>2.768374</td>\n",
              "      <td>1.677201</td>\n",
              "      <td>-0.475392</td>\n",
              "      <td>-0.835870</td>\n",
              "      <td>-1.314879</td>\n",
              "      <td>94.835887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.715734</td>\n",
              "      <td>0.383168</td>\n",
              "      <td>-0.686715</td>\n",
              "      <td>-1.236136</td>\n",
              "      <td>0.731001</td>\n",
              "      <td>1.623885</td>\n",
              "      <td>1.254338</td>\n",
              "      <td>2.394362</td>\n",
              "      <td>2.185095</td>\n",
              "      <td>0.538435</td>\n",
              "      <td>-161.502563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
              "0   0.123429  -1.253402   0.370340   0.101788   0.092628  -0.589254   \n",
              "1  -0.216827   0.214983  -0.028817  -1.701140   0.264482   0.314972   \n",
              "2   0.388979   1.655407  -0.755792  -1.161784  -0.300860   1.048707   \n",
              "3   0.219072  -0.251552  -1.095871  -0.806520  -0.435139   2.768374   \n",
              "4   0.715734   0.383168  -0.686715  -1.236136   0.731001   1.623885   \n",
              "\n",
              "   feature_7  feature_8  feature_9  feature_10      target  \n",
              "0   0.306348  -1.458213   1.630130    1.242863  -44.755165  \n",
              "1   0.374062  -0.292758   0.501900    0.063702  -16.218565  \n",
              "2  -0.283139   0.251474  -0.194269   -1.209477   -9.541669  \n",
              "3   1.677201  -0.475392  -0.835870   -1.314879   94.835887  \n",
              "4   1.254338   2.394362   2.185095    0.538435 -161.502563  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creamos un dataset de prueba\n",
        "## Utilicen mas `n_features` > 1 y n_informative mas o menos la mitad de n_features\n",
        "X, y = make_regression(n_samples = 1000,\n",
        "                       n_features = 10,\n",
        "                       noise = 100,\n",
        "                       n_informative = 5,\n",
        "                       random_state = 42)\n",
        "\n",
        "new_data = np.append(X,y.reshape(-1,1),axis=1)\n",
        "\n",
        "df = pd.DataFrame(new_data)\n",
        "\n",
        "new_col = []\n",
        "i = 0\n",
        "for col in df.columns:\n",
        "    if i ==  len(df.columns) - 1:\n",
        "        new_col.append(\"target\")\n",
        "    else :\n",
        "        new_col.append(\"feature_\" + str(i+1))\n",
        "        \n",
        "    i += 1\n",
        "    \n",
        "df.columns = new_col\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e7924e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6d1602",
      "metadata": {},
      "source": [
        "## Tercera Parte (Clase 5, 6 y 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce61f3a7",
      "metadata": {},
      "source": [
        "La ultima parte la van a poder encontrar en el archivo `template_tp_integrador_tercera_parte.ipynb`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
