{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo integrador - Parte 3\n",
        "## Aprendizaje No Supervisado\n",
        "**Nombre**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5921336",
      "metadata": {},
      "source": [
        "## Ejercio 8\n",
        "\n",
        "Para este ejercicio vamos a utilizar el dataset de _digits_ MNIST:\n",
        "\n",
        "* [MNIST](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
        "\n",
        "1. Aplicar PCA (validar que se cumplan las condiciones), ¿Cuántas componentes necesitamos para explicar el 80% de la varianza?\n",
        "2. Gráficar la variación acumulada para cada caso.\n",
        "3. Utilizando [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
        "Agrupar el dataset transformado (ejercicio de PCA) y agrupar en clusters de $k=10$ y $k=2$.\n",
        "4. Graficar los resultados con los distintos k's usando las primeras dos componentes principales como ejes x,y.\n",
        "5. Explique. ¿Cuál fue la ganancia de usar PCA en conjunto con k-means?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6603e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ff6afea2",
      "metadata": {},
      "source": [
        "## Ejercicio 9 (OPCIONAL)\n",
        "\n",
        "> ***NOTA***: Este ejercicio no tienen que entregarlo. Se los dejo para que puedan\n",
        "tenerlo de referencia!\n",
        "\n",
        "En este ejercicio vamos a crear un _clasificador_ de caras usando GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b467db6",
      "metadata": {},
      "outputs": [],
      "source": [
        "## importing basics libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## import olivetti faces dataset from sklearn\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "## import PCA, GMM and Scaler from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "## import train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## import bic from sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "## import tsne for visualization\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## some utils\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Conv2d, MaxPool2d, Linear, Flatten, Dropout, Module, Sequential, BatchNorm2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8d0e5ada",
      "metadata": {},
      "source": [
        "### Paso 1: Preparación del Conjunto de Datos\n",
        "\n",
        "1. Carga el conjunto de datos Olivetti Faces utilizando la función fetch_olivetti_faces de sklearn.\n",
        "\n",
        "2. Imprimir la forma de las imágenes y las etiquetas objetivo.\n",
        "\n",
        "3. Divide el conjunto de datos en un conjunto de entrenamiento y uno de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126dbe95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gallery(title, images, n_col=2, n_row=2):\n",
        "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
        "    plt.suptitle(title, size=16)\n",
        "    for i, comp in enumerate(images):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        vmax = max(comp.max(), -comp.min())\n",
        "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
        "                   interpolation='nearest',\n",
        "                   vmin=-vmax, vmax=vmax)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b18c61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load faces data\n",
        "dataset = fetch_olivetti_faces(shuffle = True, random_state = 42)\n",
        "faces = dataset.data\n",
        "\n",
        "n_samples, n_features = faces.shape\n",
        "\n",
        "print(\"Dataset consists of %d faces\" % n_samples)\n",
        "print(\"Every image contains %d features\" % n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09eace87",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141fa45f",
      "metadata": {},
      "outputs": [],
      "source": [
        "## split dataset into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    faces, dataset.target, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935a0f87",
      "metadata": {},
      "outputs": [],
      "source": [
        "## plotting a few faces from train\n",
        "n_col = 2\n",
        "n_row = 2\n",
        "image_shape = (64, 64)\n",
        "plot_gallery(\"First few train faces\", X_train[:n_col * n_row], n_col=n_col, n_row=n_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66384368",
      "metadata": {},
      "outputs": [],
      "source": [
        "## plot some test faces\n",
        "plot_gallery(\"First few test faces\", X_test[:n_col * n_row], n_col=n_col, n_row=n_row)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5133eed",
      "metadata": {},
      "source": [
        "### Paso 2: Visualización del dataset en el espacio de T-SNE\n",
        "\n",
        "1. Ahora que importamos la libreria de T-SNE, inicializamos la clase.\n",
        "2. Dado que queremos visualizar, deberiamos setear `n_components` en 2\n",
        "o 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1297c00d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## use tsne to visualize the embeddings for train and test\n",
        "tsne = ...\n",
        "\n",
        "## fit tsne on train and test\n",
        "X_train_tsne = tsne.fit_transform(X_train)\n",
        "X_test_tsne = tsne.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9642442",
      "metadata": {},
      "outputs": [],
      "source": [
        "## plot the embeddings\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train)\n",
        "plt.title(\"Train\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_test)\n",
        "plt.title(\"Test\")\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa7d711b",
      "metadata": {},
      "source": [
        "### Paso 3: Entrenamos GMM para cada Persona\n",
        "\n",
        "1. Vamos a crear una serie de modelos para cada target del modelo.\n",
        "2. **NOTA:** La idea es que ustedes elijan el mejor numero de componentes\n",
        "tomando por ejemplo el criterio de _BIC_\n",
        "\n",
        "**NOTA 2:** Este paso puede representar una carga muy pesada de cálculo a su\n",
        "computadora. Por ello vamos a analizar las primeras 10 caras del dataset solamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97abd8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "## filtering the first n_labels labels from the dataset\n",
        "\n",
        "n_labels = 5\n",
        "\n",
        "X_train_red = X_train[y_train <= n_labels]\n",
        "X_test_red = X_test[y_test <= n_labels]\n",
        "\n",
        "y_train_red = y_train[y_train <= n_labels]\n",
        "y_test_red = y_test[y_test <= n_labels]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "808ec1f8",
      "metadata": {},
      "source": [
        "Vamos a crear un diccionario de modelos de GMM para las $n_labels$ caras\n",
        "a analizar. La idea es que elijan la cantidad de componentes correctas.\n",
        "\n",
        "En este esqueleto dejamos fijas 5 componentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8bd715",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dictionary to store the GMM for each person\n",
        "gmms = {}\n",
        "\n",
        "for i in tqdm(range(n_labels)):\n",
        "    # Get the images of the current person\n",
        "    person_images = X_train_red[y_train_red == i]\n",
        "\n",
        "    # Train a GMM on these images\n",
        "    gmm = GaussianMixture(\n",
        "        n_components=5, random_state=42\n",
        "    )\n",
        "    gmm.fit(person_images)\n",
        "\n",
        "    # Store the GMM in the dictionary\n",
        "    gmms[i] = gmm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e6bede46",
      "metadata": {},
      "source": [
        "\n",
        "### Paso 4: Hora de predecir!!\n",
        "\n",
        "Vamos a generar las nuevas clasificaciones con nuestros modelos de \n",
        "GMM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2739e622",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70eef497",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_pred(image: pd.Series, models: Dict[int, Any]) -> Dict[str, int]:\n",
        "    best_person = None\n",
        "    best_score = -np.inf\n",
        "\n",
        "    scores = [gmm.score_samples([image]) for gmm in models.values()]\n",
        "\n",
        "    ## now that we have the scores, we should get the model that has\n",
        "    ## the highest score.\n",
        "    return {\n",
        "        \"best_person\": np.argmax(scores),\n",
        "        \"best_score\": scores[np.argmax(scores)],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50281b23",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for image in tqdm(X_test_red):\n",
        "    preds = get_best_pred(image, gmms)\n",
        "\n",
        "    predictions.append(preds)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d623eae8",
      "metadata": {},
      "source": [
        "### Paso 5: Validamos el resultado\n",
        "\n",
        "En este paso, Vamos a checkear el accuracy y el f1 de nuestro predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0d93e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Primero armamos un crosstab a ver si el modelo funciona.\n",
        "pd.crosstab(\n",
        "    y_test_red,\n",
        "    [pred[\"best_person\"] for pred in predictions],\n",
        "    rownames=[\"True\"],\n",
        "    colnames=[\"Predicted\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7997aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_score(y_test_red, [pred[\"best_person\"] for pred in predictions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849151ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_score(y_test_red, [pred[\"best_person\"] for pred in predictions], average=\"macro\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e7880851",
      "metadata": {},
      "source": [
        "### Paso 5: Conclusión\n",
        "\n",
        "1. ¿Qué aprendiste de la visualización con t-SNE? ¿Cómo se desempeñó el GMM?\n",
        "\n",
        "2. Discute las posibles mejoras que podrían realizarse en este modelo de detección de rostros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio 09 Version con CNN (OPCIONAL)\n",
        "\n",
        "En este ejercicio vamos a utilizar un clasificador basado en una red neuronal convolucional para clasificar las caras del dataset Olivetti Faces.\n",
        "\n",
        "> ***NOTA***: Este ejercicio no tienen que entregarlo. Se los dejo para que puedan\n",
        "tenerlo de referencia!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 1: Preparación del Conjunto de Datos\n",
        "\n",
        "Para este paso, lo primero que tenemos que hacer es generar los `dataloaders`\n",
        "del dataset. Esto son como los splits que conocemos, pero en un formato que\n",
        "las librerias de deep learning manejan de una mejor manera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## importamos lo necesario primero\n",
        "\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "faces = fetch_olivetti_faces()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## En este paso vamos a crear un dataset de pytorch que nos permita\n",
        "## cargar las imagenes de forma mas eficiente.\n",
        "class FacesDataset(Dataset):\n",
        "    def __init__(self, faces):\n",
        "        self.data = torch.from_numpy(faces.images).float()\n",
        "        self.targets = torch.from_numpy(faces.target).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos el dataset!\n",
        "full_dataset = FacesDataset(faces)\n",
        "\n",
        "# Seteamos los parametros para el dataloader\n",
        "num_workers = 0 \n",
        "# Tamaño del batch\n",
        "batch_size = 20 \n",
        "\n",
        "# Creamos el dataloader\n",
        "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=batch_size, num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Y ahora si! separamos los datos\n",
        "\n",
        "split_sizes = [\n",
        "    int(0.7 * len(full_dataset)),\n",
        "    int(0.15 * len(full_dataset)),\n",
        "    int(0.15 * len(full_dataset)) + 1,\n",
        "]\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, split_sizes\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2: Generación del modelo\n",
        "\n",
        "En este paso vamos a utilizar la libreria de Pytorch Lightning para generar\n",
        "nuestro modelo de clasificación. Esta libreria nos va a permitir \"despreocuparnos\"\n",
        "de ciertos métodos que deben estar presentes dentro de una red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importamos lo necesario\n",
        "\n",
        "from pytorch import nn\n",
        "from pytorch.functional import F\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## check if the OS is Mac set device to \"mps\" if not, validate if cuda is available an set device to GPU or CPU\n",
        "if sys.platform == \"darwin\":\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(64 * 15 * 15, 128)\n",
        "        self.fc2 = nn.Linear(128, 40)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 15 * 15)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = F.log_softmax(self.fc2(x), dim=1)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        loss = F.nll_loss(self(x), y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        loss = F.nll_loss(self(x), y)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        loss = F.nll_loss(self(x), y)\n",
        "        self.log('test_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Aqui vamos a cargar tensorboard, que nos va a permitir ver los resultados a medida que entrenamos\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
        "model = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualizamos nuestro modelo\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ahora si, entrenamos!\n",
        "trainer = Trainer(max_epochs=5, logger=logger)\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "trainer.test(test_dataloaders=test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3d4de736",
      "metadata": {},
      "source": [
        "## Ejercicio 10 (OPCIONAL)\n",
        "\n",
        "El objetivo de este ejercicio es utilizar GMM para agrupar documentos de texto\n",
        "basándose en sus características. Esto nos debería dar un buen pantallazo\n",
        "del uso que podemos darle a los métodos de segmentación.\n",
        "\n",
        "> ***NOTA***: Este ejercicio no tienen que entregarlo. Se los dejo para que puedan\n",
        "tenerlo de referencia!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7b85764d",
      "metadata": {},
      "source": [
        "> ***NOTA:*** Aquí vamos a utilizar HuggingFace Transformers, tengan en cuenta\n",
        "    que esto va a poner una carga posiblemente MUY grande en su computadora. \n",
        "    Si su compu no tiene muchos recursos por favor, avisenme y configuramos esto\n",
        "    en Google colab para poder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc629a92",
      "metadata": {},
      "outputs": [],
      "source": [
        "## importing basics libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "## import train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## import tsne for visualization\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "## Import GMM and PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "\n",
        "## importing transformers libraries\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "## importing spacy library and en_core_web_lg model\n",
        "import spacy\n",
        "\n",
        "## importing 20newsgroups dataset from sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "## some utils\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c56a639",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6504a0b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## check if the OS is Mac set device to \"mps\" if not, validate if cuda is available an set device to GPU or CPU\n",
        "if sys.platform == \"darwin\":\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c033a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "## initializing the pipeline called embedding using bert-model-uncased from huggingface\n",
        "## including the tokenizer and the model itself. GPU is used for faster inference if available.\n",
        "pipe = pipeline(\n",
        "    \"feature-extraction\",\n",
        "    model=\"distilbert-base-uncased\",\n",
        "    tokenizer=\"distilbert-base-uncased\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c3ee4d84",
      "metadata": {},
      "source": [
        "Para inicializar el modelo de spacy, primero tenemos que descargar el modelo. Para ello corremos el siguiente comando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efec2c2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8340d9e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Initializing the spacy model. first it should be downloaded using the following command:\n",
        "## python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b2d6c23e",
      "metadata": {},
      "source": [
        "### Paso 1: Preparación del Conjunto de Datos\n",
        "\n",
        "1. Aquí, vamos a importar las bibliotecas principales que vamos a usar,\n",
        "tales como NumPy, sklearn, pandas, etc.\n",
        "\n",
        "2. Vamos a tomar el dataset 20 Newsgroups de sklearn.\n",
        "\n",
        "3. Separación en train y test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffed6da",
      "metadata": {},
      "outputs": [],
      "source": [
        "## importing the dataset and filtering only the first 5 labels\n",
        "dataset = fetch_20newsgroups(\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    subset=\"train\",\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    categories=[\n",
        "        \"comp.graphics\",\n",
        "        \"comp.os.ms-windows.misc\",\n",
        "        \"comp.sys.ibm.pc.hardware\",\n",
        "        \"comp.sys.mac.hardware\",\n",
        "        \"comp.windows.x\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "## getting the data and perfoming some preprocessing such as removing the\n",
        "## zero length strings and removing the new line characters.\n",
        "data = dataset.data\n",
        "data = [text.replace(\"\\n\", \" \") for text in data if len(text) > 0]\n",
        "\n",
        "## now removing the labels for the texts that were removed\n",
        "labels = dataset.target[[len(text) > 0 for text in dataset.data]]\n",
        "\n",
        "## converting all the texts to lowercase\n",
        "data = [text.lower() for text in data]\n",
        "\n",
        "\n",
        "## splitting the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0157c7b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset consists of %d texts\" % len(data))\n",
        "print(\"Train set consists of %d texts\" % len(X_train))\n",
        "print(\"Test set consists of %d texts\" % len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6b427d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## print some examples\n",
        "for i in range(2):\n",
        "    print(f\"Example {i} - Label {y_train[i]}\")\n",
        "    print(X_train[i][:200])\n",
        "    print(\"=========================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## plot the histogram of the lengths of the texts\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist([len(text) for text in X_train], bins=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## plot the distributions of the labels. both in train and test on the same graph\n",
        "## on the X_label it should appear the name of the label and not the number.\n",
        "\n",
        "plt.hist(y_train, alpha = 0.5, label = \"Train\")\n",
        "plt.hist(y_test, alpha = 0.5, label = \"Test\")\n",
        "plt.xticks(range(len(dataset.target_names)), dataset.target_names, rotation = 90)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cc80c2bd",
      "metadata": {},
      "source": [
        "### Paso 2: Extracción de Características\n",
        "\n",
        "En este paso vamos a utilizar dos tipos de embeddings, los embeddings de\n",
        "transformers y los embeddings de SpaCy.\n",
        "\n",
        "> **NOTA** Este paso puede ser MUY pesado para su compu. Tengan en cuenta\n",
        "esto antes de correr estos pasos!\n",
        "\n",
        "> En el caso que no puedan correr estas lineas, pueden ignorar este paso\n",
        "y utilizar directamente los archivos que están provistos en la carpeta\n",
        "`./data/embeddings/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90b26dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_embeddings = pipe(X_train)\n",
        "test_embeddings = pipe(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02e04d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# now from embs we should get every embedding and get the mean of the embeddings\n",
        "# for each text. this should be done for both train and test sets.\n",
        "X_train_embs = np.array(\n",
        "    [emb.mean(axis=1).detach().numpy().squeeze() for emb in tqdm(train_embeddings)]\n",
        ")\n",
        "X_test_embs = np.array(\n",
        "    [emb.mean(axis=1).detach().numpy().squeeze() for emb in tqdm(test_embeddings)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e757cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Es una buena recomendación guardar los embeddings en estos momentos. Así nos evitamos volver a calcularlos\n",
        "## Guardamos entonces los embeddings en un archivo .parquet usando pandas.\n",
        "## en el archivo de pandas vamos a guardar tanto los embeddings como las etiquetas de cada texto.\n",
        "## las columnas de features deben llamarse \"emb_0\", \"emb_1\", etc.\n",
        "## para guardar los embeddings en un archivo parquet, primero debemos convertirlos a un dataframe de pandas\n",
        "## y luego usar el método to_parquet de pandas.\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_embs, columns = [f\"emb_{i}\" for i in range(X_train_embs.shape[1])])\n",
        "y_train_df = pd.DataFrame(y_train, columns = [\"target\"])\n",
        "X_test_df = pd.DataFrame(X_test_embs, columns = [f\"emb_{i}\" for i in range(X_test_embs.shape[1])])\n",
        "y_test_df = pd.DataFrame(y_test, columns = [\"target\"])\n",
        "\n",
        "pd.concat([X_train_df, y_train_df], axis = 1).to_parquet(\"./data/embeddings/train_tf.parquet\")\n",
        "pd.concat([X_test_df, y_test_df], axis = 1).to_parquet(\"./data/embeddings/test_tf.parquet\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eb6af8bf",
      "metadata": {},
      "source": [
        "Ahora que generamos los embeddings con Tranformers, hacemos lo mismo pero con SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0261fd7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para cada documento, utiliza Spacy para obtener el vector del documento\n",
        "X_train_spacy = np.array([nlp(doc).vector for doc in tqdm(X_train)])\n",
        "X_test_spacy = np.array([nlp(doc).vector for doc in tqdm(X_test)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff2ebb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_df = pd.DataFrame(X_train_spacy, columns = [f\"emb_{i}\" for i in range(X_train_spacy.shape[1])])\n",
        "y_train_df = pd.DataFrame(y_train, columns = [\"target\"])\n",
        "X_test_df = pd.DataFrame(X_test_spacy, columns = [f\"emb_{i}\" for i in range(X_test_spacy.shape[1])])\n",
        "y_test_df = pd.DataFrame(y_test, columns = [\"target\"])\n",
        "\n",
        "pd.concat([X_train_df, y_train_df], axis = 1).to_parquet(\"./data/embeddings/train_sp.parquet\")\n",
        "pd.concat([X_test_df, y_test_df], axis = 1).to_parquet(\"./data/embeddings/test_sp.parquet\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3f3025ef",
      "metadata": {},
      "source": [
        "### Paso 3: Aplicación de GMM\n",
        "\n",
        "1. Aplica el GMM a las características de obtenidas (Tanto en transformers como en SpaCy). Experimenta con diferentes números de componentes.\n",
        "\n",
        "2. Utiliza el GMM para predecir una etiqueta de cluster para cada documento."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "85b68a22",
      "metadata": {},
      "source": [
        "Primero antes de arrancar es un buen momento de visualizar nuestros embeddings. Para ello,\n",
        "primero vamos a cargar los embeddings.\n",
        "\n",
        "recordemos que los embeddings con sufijo `_tf` corresponden al modelo de Transformers y\n",
        "`_sp` corresponden al modelo de SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747785c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_parquet(\"./data/embeddings/train_tf.parquet\")\n",
        "df_test = pd.read_parquet(\"./data/embeddings/test_tf.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be17157",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd5ba79",
      "metadata": {},
      "outputs": [],
      "source": [
        "## create a visualization of the embeddings using tsne.\n",
        "## use the target to color the points.\n",
        "## use the following colors: [\"red\", \"blue\", \"green\", \"yellow\", \"orange\"]\n",
        "\n",
        "tsne = TSNE()\n",
        "X_tsne = tsne.fit_transform(df_train.drop(\"target\", axis = 1))\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c = df_train[\"target\"], cmap = \"rainbow\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bbacc8bf",
      "metadata": {},
      "source": [
        "Ahora si! vamos con GMM. Para ello primero vamos a utilizar PCA para poder\n",
        "comprimir la informacion de los embeddings en algo más maleable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67020dd9",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "925dba01",
      "metadata": {},
      "source": [
        "Ahora evaluemos, como le fue al clusterizador?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68961ab8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Creamos una tabla de frecuencias cruzadas entre las etiquetas y los clusters.\n",
        "pd.crosstab(df_pca_train['target'], df_pca_train['cluster'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c77d98ba",
      "metadata": {},
      "source": [
        "### Paso 4: Evaluación de los Resultados\n",
        "\n",
        "1. Examinar algunos documentos de cada cluster. Tienen sentido los resultados?\n",
        "\n",
        "2. Experimenta con diferentes números de clusters. Podemos encontrar algo interesante variando el parámetro\n",
        "de `n_components`?\n",
        "\n",
        "3. ¿Qué sucede si cambias el tipo de covarianza en el GMM? ¿Cómo afecta esto a los resultados?\n",
        "\n",
        "4. Si hubiesemos usado kMeans como alternativa para clusterizar, hubiesemos tenido mejor resultado?\n",
        "\n",
        "5. Entre el embedding de SpaCy y el embedding de Transformers, encontró alguna diferencia respecto\n",
        "a la clusterización?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
