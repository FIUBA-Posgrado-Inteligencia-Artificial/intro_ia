# Clase 7 - Expectation Maximization y Gaussian Mixture Models
En ésta clase se realizará una introducción al framework teórico de los modelos de aprendizaje
no supervisado, utilizando como ejemplo los Gaussian Mixture Models (GMM). Se introducirá
el concepto de variables latentes y de expectation maximization (EM) como algoritmo de aproximación
de la máxima verosimilitud. Se verán las aplicaciones de los GMM a la estimación de funciones de densidad,
clusterización, detección de anomalías, muestreo y object tracking.

## Teoría
* [Presentación](presentaciones/clase_7.pdf)
* [Jamboard Desarrollo Matemático](presentaciones/clase_7_jamboard.pdf)
* [K-means CS229: Machine Learning, Stanford](presentaciones/cs229-kmeans.pdf)
* [GMM CS229: Machine Learning, Stanford](presentaciones/cs229-gmm.pdf)
* [EM CS229: Machine Learning, Stanford](presentaciones/cs229-em.pdf)

## Notebooks
* [GMM - Anomaly Detection](jupyterbooks/GMM%20-%20Anomaly%20Detection.ipynb)
* [GMM - Density Estimation and Sampling](jupyterbooks/GMM%20-%20Density%20Estimation%20and%20Sampling.ipynb)
* [Synthetic Dataset](ejercicios/Synthetic%20Dataset.ipynb)
* [Expectation Maximization - GMM](ejercicios/Expectation%20Maximization%20-%20Gaussian%20Mixture%20Models.ipynb)
* [GMM Numpy - Synthetic Dataset](ejercicios/EM%20-%20GMM%20-%20Synthetic%20Dataset.ipynb)

## Ejercicios
* [Gaussian Mixture Models - 1D](ejercicios/GMM_1D.py)
* [Gaussian Mixture Models - Background Extraction](jupyterbooks/GMM_Background.py)
