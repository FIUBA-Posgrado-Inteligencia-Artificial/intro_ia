{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En éste notebook vamos a explorar los algoritmos de descenso por el grandiente estándar, estocástico y mini batch junto con sus implementaciones en NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for *epoch* in **n_epochs**:\n",
    "1. Calcular las predicciones para todas las muestras $\\hat{y}$\n",
    "2. Calcular el error entre la salida real y las predicciones.\n",
    "3. Calcular el gradiente usando todas las muestras.\n",
    "4. Actualizar todos los parámetros del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: Implementar GD vectorizado en NumPy\n",
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "def gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_train = nxm\n",
    "        y_train = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1) # mx1\n",
    "\n",
    "    for i in range(amt_epochs):\n",
    "        # Calculate the predictions for all samples\n",
    "        prediction = np.matmul(X_train, W)  # nx1\n",
    "        \n",
    "        # Calculate the error for all samples\n",
    "        error = y_train - prediction  # nx1\n",
    "\n",
    "        # Calculate the gradient for all samples\n",
    "        grad_sum = np.sum(error * X_train, axis=0)\n",
    "        grad_mul = -2/n * grad_sum  # 1xm\n",
    "        gradient = np.transpose(grad_mul) # mx1 (it also works with reshape)\n",
    "\n",
    "        # Update the parameters\n",
    "        W = W - (lr * gradient)\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for *epoch* in **n_epochs**:\n",
    "* Mezclar las muestras (shuffle)\n",
    "* for *sample* in **n_samples**:\n",
    "    1. Calcular la predicción para la muestra $\\hat{y_{i}}$\n",
    "    2. Calcular el error entre la salida real y la predicción.\n",
    "    3. Calcular el gradiente usando la muestra.\n",
    "    4. Actualizar todos los parámetros del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: Implementar SGD en NumPy\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "def stochastic_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_train = nxm\n",
    "        y_train = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    # Iterate over the n_epochs\n",
    "    for i in range(amt_epochs):\n",
    "        \n",
    "        # Shuffle all the samples\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        # Iterate over each sample\n",
    "        for j in range(n):\n",
    "            # Calculate the prediction for each sample\n",
    "            prediction = np.matmul(X_train[j].reshape(1, -1), W)  # 1x1\n",
    "            # Calculate the error for each sample\n",
    "            error = y_train[j] - prediction  # 1x1\n",
    "            # Calculate the gradient for each sample\n",
    "            grad_sum = error * X_train[j] # 1xm\n",
    "            grad_mul = -2/n * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
    "            # Update all the weights\n",
    "            W = W - (lr * gradient) # mx1\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for *epoch* in **n_epochs**:\n",
    "* Mezclar los **batches** (shuffle)\n",
    "* for *batch* in **n_batches**:\n",
    "    1. Calcular las predicciones para el batch $\\hat{y_{batch}}$\n",
    "    2. Calcular el error entre las salidas reales del batch y las predicciones.\n",
    "    3. Calcular el gradiente para el batch.\n",
    "    4. Actualizar todos los parámetros del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: implementar MBGD en NumPy\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100):\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "def mini_batch_gradient_descent(X_train, y_train, lr=0.01, amt_epochs=100, b=16):\n",
    "    \n",
    "    \"\"\"\n",
    "    shapes:\n",
    "        X_train = nxm\n",
    "        y_train = nx1\n",
    "        W = mx1\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    # initialize random weights\n",
    "    W = np.random.randn(m).reshape(m, 1)\n",
    "\n",
    "    # iterate over the n_epochs\n",
    "    for i in range(amt_epochs):\n",
    "        \n",
    "        # Shuffle all the samples \n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        # Calculate the batch size in samples as a function of the number of batches\n",
    "        batch_size = int(len(X_train) / b)\n",
    " \n",
    "        # Iterate over the batches\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            \n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end] # batch_size*m\n",
    "            batch_y = y_train[i: end] # batch_size*1\n",
    "\n",
    "            # Calculate the prediction for the whole batch\n",
    "            prediction = np.matmul(batch_X, W)  # batch_sizex1\n",
    "            # Calculate the error for the whole batch\n",
    "            error = batch_y - prediction  # batch_sizex1\n",
    "\n",
    "            # Calculate the gradient for the batch\n",
    "            \n",
    "            # error[batch_sizex1]*batch_X[batch_size*m]--> broadcasting --> batch_size*m\n",
    "            grad_sum = np.sum(error * batch_X, axis=0) # 1xm\n",
    "            grad_mul = -2/batch_size * grad_sum  # 1xm\n",
    "            gradient = np.transpose(grad_mul)  # mx1\n",
    "            \n",
    "            # Update the weights\n",
    "            W = W - (lr * gradient)\n",
    "\n",
    "    return W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
