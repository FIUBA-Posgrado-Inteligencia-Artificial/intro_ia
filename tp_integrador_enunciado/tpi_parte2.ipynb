{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo integrador - Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42989be4",
      "metadata": {},
      "source": [
        "# Aprendizaje Supervisado\n",
        "\n",
        "**Nombre**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8387800e",
      "metadata": {},
      "source": [
        "## Problema de regresión"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dc5e74de",
      "metadata": {},
      "source": [
        "Para la creación de los datasets y la manipulación de los mismos vamos a trabajar directamente con dos módulos includios en la carpeta utils.\n",
        "\n",
        "En esta podemos encontrar:\n",
        " - generate_data: Esta función wrappea el método de _make_regression_ de scikit learn para devolver un dataframe con un problema de regresión basado en sus parámetros.\n",
        " - generate_outliers: Esta función genera outliers livianos y pesados en función de los parámetros que le demos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a033541",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_generation import generate_dataset\n",
        "from utils.data_manipulation import generate_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc333db",
      "metadata": {},
      "source": [
        "### Ejemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a902d24a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee49e6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Vamos a crear un dataset primero.\n",
        "\n",
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=5,\n",
        "    n_informative=2,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n",
        "\n",
        "## esto nos genera un dataset que contiene 5 features, 2 de los cuales son informativos, y 1 target.\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0df0dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas variables\n",
        "## creamos una figura de matplotlib que contenga 5 subplots, uno por cada feature:\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
        "\n",
        "## Creamos un loop para iterar sobre cada feature y graficar la regresión lineal entre cada feature y el target:\n",
        "\n",
        "for i, feature in enumerate(data.columns[:-1]):\n",
        "    sns.regplot(x=feature,\n",
        "                y='target',\n",
        "                data=data,\n",
        "                ax=axes[i],\n",
        "                scatter_kws={'alpha': 0.4},\n",
        "                line_kws={'color': 'red'},\n",
        "                ci=95)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9d4947",
      "metadata": {},
      "source": [
        "Ahora agregamos _outliers_ a un nuevo dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fcec748",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = generate_dataset(\n",
        "    n_samples=1000,\n",
        "    n_features=1,\n",
        "    n_informative=1,\n",
        "    n_targets=1,\n",
        "    noise=0,\n",
        "    output='dataframe'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f706cc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "do1 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=False,\n",
        ")\n",
        "do2 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=True,\n",
        ")\n",
        "do3 = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.01,\n",
        "    extreme_outliers=False,\n",
        "    only_tails=True,\n",
        "    two_tailed=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37444567",
      "metadata": {},
      "outputs": [],
      "source": [
        "## vamos a visualizar estas los distintos datasets\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do1,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do2,\n",
        "            ax=axes[2],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[2].set_title('Outliers (two-tailed)')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=do3,\n",
        "            ax=axes[3],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[3].set_title('Outliers (one-tailed)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcda0e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## y si lo queremos con mucho mas outliers?\n",
        "\n",
        "doe = generate_outliers(\n",
        "    df=data,\n",
        "    columns=['x0'],\n",
        "    percentage=0.1,\n",
        "    extreme_outliers=True)\n",
        "\n",
        "## vamos a visualizar este caso\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=data,\n",
        "            ax=axes[0],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[0].set_title('Original')\n",
        "\n",
        "sns.regplot(x='x0',\n",
        "            y='target',\n",
        "            data=doe,\n",
        "            ax=axes[1],\n",
        "            scatter_kws={'alpha': 0.4},\n",
        "            line_kws={'color': 'red'},\n",
        "            ci=95)\n",
        "axes[1].set_title('Outliers')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2734dfde",
      "metadata": {},
      "source": [
        "## Ejercicio 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44621118",
      "metadata": {},
      "source": [
        "Utilizando la funcion `generate_data` generar un problema de regresión multivariada en el cual cuente con N variables informativas y M variables no informativas.\n",
        "\n",
        "Ejemplo:\n",
        "```python\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=10,\n",
        "                    n_informative=5,\n",
        "                    n_targets=1,\n",
        "                    noise=20.0,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "```\n",
        "\n",
        "Dado un valor de _noise_ fijo, sin fijar _random_state_ (para poder asegurarnos\n",
        "que los datos que generamos son distintos) realizaremos 100 simulaciones de este dataset.\n",
        "\n",
        "En la simulación deberemos generar el dataset, hacer una división de train-test, ajustar\n",
        "un modelo de regresión lineal multivariada y validar el mismo.\n",
        "\n",
        "En cada iteración de esta simulación debemos guardar:\n",
        "\n",
        "- Los coeficientes de la regresión.\n",
        "- El RMSE de train y test.\n",
        "- El MAE de train y test. \n",
        "\n",
        "\n",
        "> Qué pasa con los coeficientes de las variables no informativas? La regresión se ve afectada por estas variables?\n",
        "> ***HINT:*** Utilice las distribuciones de los coeficientes para analizar y test de hipótesis para sacar conclusiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa01488",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df91762",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Parámetros de la simulación\n",
        "n_exp = 1000\n",
        "n_samples = 1000\n",
        "n_features = 10\n",
        "n_informative = 2\n",
        "n_targets = 1\n",
        "\n",
        "noise = np.linspace(0, 100, 100)\n",
        "bias = np.linspace(0, 100, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e0cf86",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Esqueleto de la simulación\n",
        "for _ in tqdm(range(n_exp)):\n",
        "    for b in bias:\n",
        "        for n in noise:\n",
        "            data = generate_dataset(\n",
        "                n_samples=n_samples,\n",
        "                n_features=n_features,\n",
        "                n_informative=n_informative,\n",
        "                n_targets=n_targets,\n",
        "                noise=n,\n",
        "                bias=b,\n",
        "                output='dataframe'\n",
        "            )\n",
        "            ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5772f3b8",
      "metadata": {},
      "source": [
        "## Ejercicio 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ad2f9ee6",
      "metadata": {},
      "source": [
        "\n",
        "Utilizando la funcion `generate_outliers` generar puntos extremos dentro de los datos que generamos anteriormente. En este ejercicio dejar setteado `extreme_outliers` como `False` y observe como variando el porcentaje de los mismos la regresión comienza a afectarse.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Generamos un dataset de regresion lineal simple (1 feature y 1 target value) con `noise` fijo en 0.5.\n",
        "2. Generamos outliers fijando `extreme_outliers`.\n",
        "2. Probar los distintos regresores a ver como se comportan frente a estos datasets anómalos.\n",
        "3. Simular con multiples porcentajes de outliers (desde 1% hasta 10%). Qué pasa con los modelos?\n",
        "\n",
        "Los modelos a utilizar en este problema son:\n",
        "\n",
        "    - Regresion Lineal simple\n",
        "    - Regresion de Huber\n",
        "    - Regresión Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a019af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, HuberRegressor, RidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7e8c85",
      "metadata": {},
      "source": [
        "## Problema de Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53495bd5",
      "metadata": {},
      "source": [
        "### Ejercicio 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9639ca5",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a jugar un poco con descenso de gradiente. Para esto consideremos lo visto en clase que es el problema de regresión.\n",
        "\n",
        "Como paso inicial, vamos a sacarnos de encima la parte teórica. Recordemos que partimos del siguiente modelo\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 \\cdot x\n",
        "$$\n",
        "\n",
        "En este caso nuestra función objetivo a optimizar será:\n",
        "\n",
        "$$\n",
        "MSE = ||y-\\hat{y}||^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6deb44f7",
      "metadata": {},
      "source": [
        "Para calcular el gradiente de la función de error cuadrático medio (MSE) con respecto a los parámetros $\\beta_0$ y $\\beta_1$, es útil primero expresar la función de coste de forma más explicita. Dado que $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$, podemos reescribir la función MSE como sigue:\n",
        "\n",
        "$$\n",
        "MSE(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)^2\n",
        "$$\n",
        "\n",
        "Aquí, $N$ es el número de observaciones en el conjunto de datos y $y_i$ y $x_i$ son el valor observado y el valor de la característica correspondiente para la i-ésima observación.\n",
        "\n",
        "El gradiente de la función de coste está compuesto por las derivadas parciales de la función de coste con respecto a cada uno de los parámetros. Así, el gradiente es un vector de la forma:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{\\partial MSE}{\\partial \\beta_0}, \\frac{\\partial MSE}{\\partial \\beta_1} \\right]\n",
        "$$\n",
        "\n",
        "Las derivadas parciales se pueden calcular como sigue:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_0} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i)\n",
        "$$\n",
        "\n",
        "Así que finalmente tenemos:\n",
        "\n",
        "$$\n",
        "\\nabla MSE(\\beta_0, \\beta_1) = \\left[ \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - \\beta_0 - \\beta_1 \\cdot x_i), \\frac{-2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i) \\right]\n",
        "$$\n",
        "\n",
        "El cálculo del gradiente se usa en el descenso de gradiente para actualizar los parámetros $\\beta_0$ y $\\beta_1$ en cada iteración, en dirección opuesta al gradiente, para minimizar la función de coste.\n",
        "\n",
        "Estos cálculos se pueden implementar en código Python de la siguiente manera:\n",
        "\n",
        "```python\n",
        "def gradient(X, y, beta0, beta1):\n",
        "    N = len(y)\n",
        "    y_hat = beta0 + beta1 * X\n",
        "\n",
        "    d_beta0 = (-2/N) * np.sum(y - y_hat)\n",
        "    d_beta1 = (-2/N) * np.sum(X * (y - y_hat))\n",
        "\n",
        "    return d_beta0, d_beta1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7dee2",
      "metadata": {},
      "source": [
        "Ahora, si quisieramos realizar esto de manera matricial, podemos hacer lo siguiente:\n",
        "\n",
        "Primero, necesitamos cambiar la representación de nuestros datos. Podemos agregar un vector de unos a nuestra matriz de características para representar el término de intersección $\\beta_0$. De esta manera, $X$ toma esta forma:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Y nuestro vector de parámetros $\\theta$ se verá así:\n",
        "\n",
        "$$\n",
        "\\theta = \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Entonces, nuestra predicción $\\hat{y}$ se calcula como $X\\theta$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X\\theta = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_N \\\\\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Nuestra función de coste MSE se ve de la siguiente manera en forma matricial:\n",
        "\n",
        "$$\n",
        "MSE(\\theta) = \\frac{1}{N} (y - X\\theta)^T (y - X\\theta)\n",
        "$$\n",
        "\n",
        "Las derivadas parciales de esta función de coste con respecto a los parámetros se pueden calcular de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial \\theta} = \\frac{-2}{N} X^T (y - X\\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4a59fb",
      "metadata": {},
      "source": [
        "\n",
        "Esto se puede implementar en Python de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ac568e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Esta función calcula el gradiente de la función de coste del error cuadrático medio (MSE)\n",
        "    para una regresión lineal simple. La función toma como entrada la matriz de características X,\n",
        "    el vector de observaciones y y el vector de parámetros theta, y devuelve el gradiente, que\n",
        "    es un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Params:\n",
        "    X : numpy.ndarray\n",
        "        La matriz de características extendida que incluye un vector de unos. De tamaño (N, d),\n",
        "        donde N es el número de observaciones y d es el número de características (incluyendo el\n",
        "        término de intersección).\n",
        "\n",
        "    y : numpy.ndarray\n",
        "        El vector de observaciones. De tamaño (N,), donde N es el número de observaciones.\n",
        "\n",
        "    theta : numpy.ndarray\n",
        "        El vector de parámetros. De tamaño (d,), donde d es el número de características\n",
        "        (incluyendo el término de intersección).\n",
        "\n",
        "    Returns:\n",
        "    grad : numpy.ndarray\n",
        "        El gradiente de la función de coste. Un vector de las mismas dimensiones que theta.\n",
        "\n",
        "    Examples:\n",
        "    >>> X = np.array([[1, 1], [1, 2], [1, 3]])\n",
        "    >>> y = np.array([2, 3, 4])\n",
        "    >>> theta = np.array([0, 0])\n",
        "    >>> gradient(X, y, theta)\n",
        "    array([-4., -8.])\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    y_hat = X.dot(theta)\n",
        "\n",
        "    grad = (-2 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ef2b1d",
      "metadata": {},
      "source": [
        "\n",
        "Aquí, `X` es la matriz de características extendida que incluye un vector de unos, `y` es el vector de observaciones, y `theta` es el vector de parámetros. La función devuelve el gradiente, que es un vector de las mismas dimensiones que `theta`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0885fd5a",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e4a496",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _GD_ que compute el gradiente descendente. Debe tener condición de frenado\n",
        "por nr de épocas pero también por tolerancia.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0e02fba8",
      "metadata": {},
      "source": [
        "#### Gradiente Descendente Estocástico"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f767cd27",
      "metadata": {},
      "source": [
        "Ahora que sabemos calcular el gradiente, vamos a:\n",
        "\n",
        "1. Crear una función _SGD_ que compute el gradiente descendente estocástico.\n",
        "2. Generamos un dataset (con _generate_dataset_ de los ejercicios anteriores, utilizando un bias conocido y solo 1 feature)\n",
        "3. Inicializamos un vector $(\\beta_0, \\beta_1)$ al azar.\n",
        "4. Tratamos de calcular los mejores parámetros con el algoritmo.\n",
        "5. Guardamos la función de perdida en train y test en cada época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e7828d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e4c183e0",
      "metadata": {},
      "source": [
        "Analice:\n",
        "\n",
        "1. Cómo se comportan estos algoritmos? se puede ver la diferencia entre SGD y GD?.\n",
        "2. Cómo afecto el _learning rate_ a estos algoritmos? Realice una simulación del mismo cambiando el `lr`.\n",
        "3. Compare en una curva de Perdida vs Epoch los dos algoritmos. Nota algo interesante?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7686a399",
      "metadata": {},
      "source": [
        "### Ejercicio 7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a07e7e1",
      "metadata": {},
      "source": [
        "En este ejercicio vamos a considerar la regresión logística como un problema de clasificación binaria.\n",
        "La implementación de la misma podemos considerar la siguiente:\n",
        "\n",
        "```python\n",
        "\n",
        "class LogisticRegressionSGD():\n",
        "    def __init__(self, lr=0.01, max_iter=1000, tol=1e-3, random_state=42):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "        self.loss = None\n",
        "        self.loss_history = None\n",
        "        self.grad_history = None\n",
        "        self.theta_history = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        self : LogisticRegressionSGD\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.weights = np.random.normal(size=X.shape[1])\n",
        "        self.loss_history = []\n",
        "        self.grad_history = []\n",
        "        self.theta_history = []\n",
        "\n",
        "        self.SGD(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _step(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform a single gradient step.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        loss : float\n",
        "            The value of the loss function for the current value of the weights.\n",
        "\n",
        "        grad : numpy.ndarray\n",
        "            The gradient of the loss function for the current value of the weights.\n",
        "        \"\"\"\n",
        "        N = len(y)\n",
        "        y_hat = self.logit(X)\n",
        "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "        grad = (-1 / N) * X.T.dot(y - y_hat)\n",
        "\n",
        "        return loss, grad\n",
        "\n",
        "    def SGD(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform the stochastic gradient descent optimization algorithm.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def logit(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the logit of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        logit : numpy.ndarray\n",
        "            The logit of the observations. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-X.dot(self.weights)))\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of each class for a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        proba : numpy.ndarray\n",
        "            The predicted probability of each class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return self.logit(X)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class of a set of observations.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        y_pred : numpy.ndarray\n",
        "            The predicted class. An array of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model.\n",
        "\n",
        "        Params:\n",
        "        X : numpy.ndarray\n",
        "            The training input samples. A 2D array of shape (n_samples, n_features).\n",
        "\n",
        "        y : numpy.ndarray\n",
        "            The target values. An array of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "        score : float\n",
        "            The accuracy of the model.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con esta clase, vamos a tomar el dataset de breast cancer y vamos a realizar una clasificación binaria.\n",
        "La idea de este ejercicio es que puedan jugar con la manera de obtener los hiperparámetros óptimos para el modelo.\n",
        "\n",
        "Para ello van a tener que completar el método `SGD` de la clase `LogisticRegressionSGD` y luego realizar una búsqueda de grilla para encontrar los mejores hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc71d51",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
